{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A tecnologia escolhida foi HDFS para armazenar os arquivos, PySpark (Jupyter) para ingestão de dados e processamento e armazenamento no MySql.\n",
    "A razão por se escolher o Py - Spark é de ser multi-funções: \n",
    "   * podemos executar comandos python;\n",
    "   * fazer ingestão de dados no spark;\n",
    "   * os selects no dataframe fazem o map-reduce (sou mais familirizado com SQL);\n",
    "   * a possibilidade de ja armazenar os dados diretamente;\n",
    "\n",
    "1) Baixar arquivos do link e armazenar no HDFS;\n",
    "2) Pode se tratar antes os arquivos e conteúdo com o python (porem neste caso tratei direto no Dataframe)\n",
    "3) Ler arquivos CSV / JSON no Spark em um DataFrame;\n",
    "4) Modificar o nome das colunas, removendo espaços e acentos (pyspark);\n",
    "5) Modificar coluna de valor trocando virgula por ponto (pyspark);\n",
    "6) Criar uma tempview no spark a partir do daframe ja tratado, filtrando somente salario maternidade;\n",
    "7) Fazer um select com group by por Ano e obtendo a o SUM(qtd de beneficios) e SUM(valor), guardando num dataframe;\n",
    "8) Criar database e tabela no MySQL com as colunas solicitadas no exercicio: indicador, ano, valor_medio (mesmo nome das colunas do dataframe);\n",
    "9) Conectar no MySQL e armazenar o Dataframe na tabela criada (cheguei até esse ponto, erro na configuração do JDBC)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ex1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfSalMater = spark.read.csv('hdfs:///user/hduser/spark/project/MAN01.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17555"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSalMater.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ano: string (nullable = true)\n",
      " |-- Especie: string (nullable = true)\n",
      " |-- Unidade_da_Federacao: string (nullable = true)\n",
      " |-- Clientela: string (nullable = true)\n",
      " |-- Quantidade_Beneficios_Mantidos: string (nullable = true)\n",
      " |-- Vlr_Benef_Mantidos: string (nullable = true)\n",
      " |-- Qte_e_Valor: string (nullable = true)\n",
      " |-- Grupo_Principais_Especies: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "dfSalMater = dfSalMater.toDF(*(re.sub(r'[\\.\\s]+', '_', c) for c in dfSalMater.columns))\n",
    "dfSalMater = dfSalMater.toDF(*(re.sub(r'í', 'i', c) for c in dfSalMater.columns))\n",
    "dfSalMater = dfSalMater.toDF(*(re.sub(r'é', 'e', c) for c in dfSalMater.columns))\n",
    "dfSalMater = dfSalMater.toDF(*(re.sub(r'çã', 'ca', c) for c in dfSalMater.columns))\n",
    "dfSalMater = dfSalMater.toDF(*(re.sub(r'_\\(R\\$\\)', '', c) for c in dfSalMater.columns))\n",
    "dfSalMater = dfSalMater.toDF(*(re.sub(r'/', '_', c) for c in dfSalMater.columns))\n",
    "dfSalMater.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import  *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ano: string (nullable = true)\n",
      " |-- Especie: string (nullable = true)\n",
      " |-- Unidade_da_Federacao: string (nullable = true)\n",
      " |-- Clientela: string (nullable = true)\n",
      " |-- Quantidade_Beneficios_Mantidos: string (nullable = true)\n",
      " |-- Vlr_Benef_Mantidos: double (nullable = true)\n",
      " |-- Qte_e_Valor: string (nullable = true)\n",
      " |-- Grupo_Principais_Especies: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf = UserDefinedFunction(lambda x: x.replace(\",\",\".\"), StringType())\n",
    "dfSalMater = dfSalMater.withColumn(\"Vlr_Benef_Mantidos\", udf(col(\"Vlr_Benef_Mantidos\")).cast(DoubleType()))\n",
    "dfSalMater.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+---------+------------------------------+------------------+--------------------+-------------------------+\n",
      "| Ano|             Especie|Unidade_da_Federacao|Clientela|Quantidade_Beneficios_Mantidos|Vlr_Benef_Mantidos|         Qte_e_Valor|Grupo_Principais_Especies|\n",
      "+----+--------------------+--------------------+---------+------------------------------+------------------+--------------------+-------------------------+\n",
      "|1988|42-Ap Tempo Contr...|             Alagoas|   Urbana|                          6205|               0.2|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|            Amazonas|   Urbana|                          3076|              0.13|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|               Bahia|   Urbana|                         31049|              1.27|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|               Ceará|   Urbana|                         14704|              0.64|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|  Mato Grosso do Sul|   Urbana|                          3232|              0.17|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|      Espírito Santo|   Urbana|                         10178|              0.47|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|               Goiás|   Urbana|                          4631|              0.23|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|            Maranhão|   Urbana|                          5650|               0.2|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|         Mato Grosso|   Urbana|                          1150|              0.05|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|        Minas Gerais|   Urbana|                         95649|              4.33|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|                Pará|   Urbana|                          9861|              0.39|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|             Paraíba|   Urbana|                          6721|              0.23|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|              Paraná|   Urbana|                         38221|              1.62|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|          Pernambuco|   Urbana|                         31966|              1.12|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|               Piauí|   Urbana|                          3136|              0.13|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|      Rio de Janeiro|   Urbana|                        238966|             10.91|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...| Rio Grande do Norte|   Urbana|                          5541|              0.22|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|   Rio Grande do Sul|   Urbana|                        104087|              4.19|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|      Santa Catarina|   Urbana|                         35239|               1.3|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "|1988|42-Ap Tempo Contr...|           São Paulo|   Urbana|                        425672|             18.04|Quantidade Benefí...|     42-Ap Tempo Contr...|\n",
      "+----+--------------------+--------------------+---------+------------------------------+------------------+--------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSalMater.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfSalMater.createOrReplaceTempView(\"salario_mater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ano: string (nullable = true)\n",
      " |-- Especie: string (nullable = true)\n",
      " |-- Unidade_da_Federacao: string (nullable = true)\n",
      " |-- Clientela: string (nullable = true)\n",
      " |-- Quantidade_Beneficios_Mantidos: string (nullable = true)\n",
      " |-- Vlr_Benef_Mantidos: double (nullable = true)\n",
      " |-- Qte_e_Valor: string (nullable = true)\n",
      " |-- Grupo_Principais_Especies: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSalMater.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+---------+------------------------------+------------------+--------------------+-------------------------+\n",
      "| Ano|             Especie|Unidade_da_Federacao|Clientela|Quantidade_Beneficios_Mantidos|Vlr_Benef_Mantidos|         Qte_e_Valor|Grupo_Principais_Especies|\n",
      "+----+--------------------+--------------------+---------+------------------------------+------------------+--------------------+-------------------------+\n",
      "|1992|80-Salário-Matern...|             Alagoas|   Urbana|                             4|              0.76|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|             Alagoas|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|            Amazonas|   Urbana|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|            Amazonas|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|               Bahia|   Urbana|                            81|             15.38|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|               Bahia|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|               Ceará|   Urbana|                             8|              1.52|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|               Ceará|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|  Mato Grosso do Sul|   Urbana|                            32|              6.08|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|  Mato Grosso do Sul|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|      Espírito Santo|   Urbana|                            32|              6.08|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|      Espírito Santo|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|               Goiás|   Urbana|                            12|              2.28|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|               Goiás|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|            Maranhão|   Urbana|                             3|              0.57|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|            Maranhão|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|         Mato Grosso|   Urbana|                            10|               1.9|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|         Mato Grosso|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|        Minas Gerais|   Urbana|                           460|             87.74|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "|1992|80-Salário-Matern...|        Minas Gerais|    Rural|                             0|               0.0|Quantidade Benefí...|     80-Salário-Matern...|\n",
      "+----+--------------------+--------------------+---------+------------------------------+------------------+--------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSQLSalMater = spark.sql(\"select * from salario_mater where Especie = '80-Salário-Maternidade' and Ano is not null\")\n",
    "dfSQLSalMater.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfMediaMaternidade = spark.sql(\"select 'salario_maternidade' as indicador , \"\n",
    "                               \" Ano as ano,  round((sum_valor / sum_qtd),5) as valor_medio from \"\n",
    "          \"(SELECT Ano , \"\n",
    "            \"    round(sum(Quantidade_Beneficios_Mantidos),5) as sum_qtd, \"\n",
    "            \"    round(sum(Vlr_Benef_Mantidos), 5) as sum_valor\"\n",
    "            \"  from salario_mater \"\n",
    "            \"  group by Ano )\"\n",
    "            \" order by Ano \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+\n",
      "|          indicador| ano|valor_medio|\n",
      "+-------------------+----+-----------+\n",
      "|salario_maternidade|   -|       null|\n",
      "|salario_maternidade|1988|     2.0E-5|\n",
      "|salario_maternidade|1989|     4.0E-4|\n",
      "|salario_maternidade|1990|    0.00443|\n",
      "|salario_maternidade|1991|    0.02025|\n",
      "|salario_maternidade|1992|    0.30531|\n",
      "|salario_maternidade|1993|   11.02098|\n",
      "|salario_maternidade|1994|  111.49281|\n",
      "|salario_maternidade|1995|  163.88484|\n",
      "|salario_maternidade|1996|  193.49026|\n",
      "|salario_maternidade|1997|  216.31316|\n",
      "|salario_maternidade|1998|  234.86222|\n",
      "|salario_maternidade|1999|  248.14432|\n",
      "|salario_maternidade|2000|  261.90032|\n",
      "|salario_maternidade|2001|  292.93459|\n",
      "|salario_maternidade|2002|  327.63214|\n",
      "|salario_maternidade|2003|  402.22265|\n",
      "+-------------------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfMediaMaternidade.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o412.save.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:38)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:78)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:78)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:78)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:55)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:472)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7e387fbecd96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mdbtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'beneficio_medio_ano'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       password='hadoop').mode('append').save();\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o412.save.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:38)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:78)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:78)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:78)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:55)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:472)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# erro neste ponto: não foi possivel fazer a conexão de JDBC por algum problema de configuração\n",
    "# Py4JJavaError: An error occurred while calling o412.save. java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n",
    "dfMediaMaternidade.write.format('jdbc').options(\n",
    "      url='jdbc:mysql://localhost:3306/proj01',\n",
    "      driver='com.mysql.jdbc.Driver',\n",
    "      dbtable='beneficio_medio_ano',\n",
    "      user='root',\n",
    "      password='hadoop').mode('append').save();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A seguir logica similar para o arquivo JSON\n",
    "Ler do HDFS em Dataframe\n",
    "Ver funções do Python / Spark para converter os elementos do JSON em dados\n",
    "Converter o DF em Temp View\n",
    "Fazer o select da tempview fazendo o SUM com group by, e depois um select externo obtendo a média\n",
    "Armazenar o resultado num DF\n",
    "Armazenar o DF numa tabela do MySQL / database proj01 / tabela 2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfBenef = spark.read.json('hdfs:///user/hduser/spark/project/CON09.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBenef.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nodes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- node: struct (nullable = true)\n",
      " |    |    |    |-- Ano: string (nullable = true)\n",
      " |    |    |    |-- Clientela: string (nullable = true)\n",
      " |    |    |    |-- Evolução Mensal (AnoMes): string (nullable = true)\n",
      " |    |    |    |-- Grupo/Principais Espécies: string (nullable = true)\n",
      " |    |    |    |-- Mês: string (nullable = true)\n",
      " |    |    |    |-- Qte Benefícios Concedidos: string (nullable = true)\n",
      " |    |    |    |-- Vlr Benefícios Concedidos (R$): string (nullable = true)\n",
      " |-- total: struct (nullable = true)\n",
      " |    |-- Ano: string (nullable = true)\n",
      " |    |-- Clientela: string (nullable = true)\n",
      " |    |-- Evolução Mensal (AnoMes): string (nullable = true)\n",
      " |    |-- Mês: string (nullable = true)\n",
      " |    |-- Qte Benefícios Concedidos: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfBenef.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
